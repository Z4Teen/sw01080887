{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f7ac6be",
   "metadata": {},
   "source": [
    "GROUP NAME AND ID\n",
    "\n",
    "NAME: Zulhakim Bin Zulkefli\n",
    "ID: SW01080887\n",
    "Section: 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbf6c930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Z4Teen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Z4Teen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Z4Teen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# For text preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# For topic modeling\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "import pandas as pd\n",
    "from gensim.models import CoherenceModel  \n",
    "\n",
    "# Download NLTK Resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import re\n",
    "import string \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1997721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data\\\\news_dataset.csv')\n",
    "documents = data['text'].dropna().tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "959451ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # Create a set of English stopwords\n",
    "lemmatizer = WordNetLemmatizer() # Initialize a WordNet lemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "296dd730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove excessive whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove words that have numbers\n",
    "    text = re.sub(r'\\b\\w*\\d\\w*\\b', '', text)\n",
    "    # Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text) \n",
    "    # Tokenize the text into words and convert to lowercase\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    # Filter out non-alphanumeric tokens\n",
    "    tokens = [token for token in tokens if token.isalnum()]\n",
    "    # Remove stopwords from the tokens\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    # Lemmatize each token\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    # Remove single-character tokens\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791c2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove numbers and hyphens\n",
    "def remove_numbers(text):\n",
    "    # Check if the input is a string\n",
    "    if isinstance(text, str):\n",
    "        # If it's a string, remove numbers and hyphens\n",
    "        return re.sub(\"[\\d-]\",'', str(text))\n",
    "    else:\n",
    "        # If it's not a string (e.g., NaN), return an empty string\n",
    "        return ''\n",
    "\n",
    "\n",
    "# Function to remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return ''.join([char for char in text if char not in string.punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c46be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the text data\n",
    "data['clean_lower'] = data['text'].str.lower()\n",
    "data['clean_number'] = data['clean_lower'].apply(remove_numbers)\n",
    "data['clean_punctuation'] = data['clean_number'].apply(remove_punctuation)\n",
    "\n",
    "# Apply preprocessing to the clean text\n",
    "data['preprocessed_text'] = data['clean_punctuation'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45ebaf66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'sport', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please']\n"
     ]
    }
   ],
   "source": [
    "preprocessed_documents = [preprocess_text(doc) for doc in documents] \n",
    "# Preprocess each document in the list\n",
    "\n",
    "print(preprocessed_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42de683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Gensim Dictionary object from the preprocessed documents\n",
    "dictionary = corpora.Dictionary(preprocessed_documents)\n",
    "# Filter out tokens that appear in less than 15 documents or more than 50% of the documents\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5)\n",
    "# Convert each preprocessed document into a bag-of-words representation using the dictionary\n",
    "corpus = [dictionary.doc2bow(doc) for doc in preprocessed_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b16fe92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5361455647850619\n"
     ]
    }
   ],
   "source": [
    "# Run LDA\n",
    "lda_model = LdaModel(corpus, num_topics=4, id2word=dictionary, passes=15) \n",
    "# Train an LDA model on the corpus with 2 topics using Gensim's LdaModel class\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=preprocessed_documents, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4bdc842c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table with Articles and Topic:\n",
      "                                                 Article  Topic\n",
      "0      I was wondering if anyone out there could enli...      1\n",
      "1      I recently posted an article asking what kind ...      1\n",
      "2      \\nIt depends on your priorities.  A lot of peo...      1\n",
      "3      an excellent automatic can be found in the sub...      1\n",
      "4      : Ford and his automobile.  I need information...      1\n",
      "...                                                  ...    ...\n",
      "11091  Secrecy in Clipper Chip\\n\\nThe serial number o...      0\n",
      "11092  Hi !\\n\\nI am interested in the source of FEAL ...      0\n",
      "11093  The actual algorithm is classified, however, t...      0\n",
      "11094  \\n\\tThis appears to be generic calling upon th...      1\n",
      "11095  \\nProbably keep quiet and take it, lest they g...      1\n",
      "\n",
      "[11096 rows x 2 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# empty list to store dominant topic labels for each document\n",
    "article_labels = []\n",
    "# iterate over each processed document\n",
    "for i, doc in enumerate(preprocessed_documents):\n",
    " # for each document, convert to bag-of-words representation\n",
    " bow = dictionary.doc2bow(doc)\n",
    " # get list of topic probabilities\n",
    " topics = lda_model.get_document_topics(bow)\n",
    " # determine topic with highest probability\n",
    " dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    " # append to the list\n",
    " article_labels.append(dominant_topic)\n",
    "    \n",
    "# Create DataFrame\n",
    "data_result = pd.DataFrame({\"Article\": documents, \"Topic\": article_labels})\n",
    "# Print the DataFrame\n",
    "print(\"Table with Articles and Topic:\")\n",
    "print(data_result)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aaeb0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Terms for Each Topic:\n",
      "Topic 0:\n",
      "- \"key\" (weight: 0.013)\n",
      "- \"use\" (weight: 0.010)\n",
      "- \"file\" (weight: 0.009)\n",
      "- \"system\" (weight: 0.009)\n",
      "- \"chip\" (weight: 0.007)\n",
      "- \"program\" (weight: 0.006)\n",
      "- \"db\" (weight: 0.006)\n",
      "- \"encryption\" (weight: 0.006)\n",
      "- \"information\" (weight: 0.006)\n",
      "- \"window\" (weight: 0.006)\n",
      "\n",
      "Topic 1:\n",
      "- \"would\" (weight: 0.015)\n",
      "- \"one\" (weight: 0.014)\n",
      "- \"know\" (weight: 0.009)\n",
      "- \"think\" (weight: 0.009)\n",
      "- \"like\" (weight: 0.009)\n",
      "- \"get\" (weight: 0.007)\n",
      "- \"say\" (weight: 0.007)\n",
      "- \"people\" (weight: 0.007)\n",
      "- \"thing\" (weight: 0.006)\n",
      "- \"god\" (weight: 0.006)\n",
      "\n",
      "Topic 2:\n",
      "- \"max\" (weight: 0.011)\n",
      "- \"people\" (weight: 0.011)\n",
      "- \"government\" (weight: 0.009)\n",
      "- \"law\" (weight: 0.007)\n",
      "- \"state\" (weight: 0.006)\n",
      "- \"would\" (weight: 0.006)\n",
      "- \"right\" (weight: 0.006)\n",
      "- \"president\" (weight: 0.006)\n",
      "- \"armenian\" (weight: 0.005)\n",
      "- \"said\" (weight: 0.005)\n",
      "\n",
      "Topic 3:\n",
      "- \"game\" (weight: 0.016)\n",
      "- \"team\" (weight: 0.013)\n",
      "- \"year\" (weight: 0.013)\n",
      "- \"player\" (weight: 0.008)\n",
      "- \"play\" (weight: 0.007)\n",
      "- \"new\" (weight: 0.007)\n",
      "- \"last\" (weight: 0.006)\n",
      "- \"first\" (weight: 0.006)\n",
      "- \"season\" (weight: 0.005)\n",
      "- \"league\" (weight: 0.005)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Top Terms for Each Topic:\")\n",
    "for idx, topic in lda_model.print_topics():\n",
    "    print(f\"Topic {idx}:\")\n",
    "    terms = [term.strip() for term in topic.split(\"+\")]\n",
    "    for term in terms:\n",
    "        weight, word = term.split(\"*\")\n",
    "        print(f\"- {word.strip()} (weight: {weight.strip()})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "646a1d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms for Topic #0:\n",
      "['key', 'use', 'file', 'system', 'chip', 'program', 'db', 'encryption', 'information', 'window']\n",
      "\n",
      "Top terms for Topic #1:\n",
      "['would', 'one', 'know', 'think', 'like', 'get', 'say', 'people', 'thing', 'god']\n",
      "\n",
      "Top terms for Topic #2:\n",
      "['max', 'people', 'government', 'law', 'state', 'would', 'right', 'president', 'armenian', 'said']\n",
      "\n",
      "Top terms for Topic #3:\n",
      "['game', 'team', 'year', 'player', 'play', 'new', 'last', 'first', 'season', 'league']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print top terms for each topic\n",
    "for topic_id in range(lda_model.num_topics):\n",
    " print(f\"Top terms for Topic #{topic_id}:\")\n",
    " top_terms = lda_model.show_topic(topic_id, topn=10)\n",
    " print([term[0] for term in top_terms])\n",
    " print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b37d98",
   "metadata": {},
   "source": [
    "Coherence score assess the interpretability of the topics generated by the topic modelling algorithm. It does this by measuring\n",
    "how semantically coherent the top words with each topic. In this case coherence score is around 0.57 so that means on average\n",
    "around 57% of the top words are semantically related.\n",
    "\n",
    "As for the topic results:\n",
    "\n",
    "Topic 1: Technology and Software. \n",
    "We can see words like 'system', 'encryption', and 'chip' are grouped together.\n",
    "\n",
    "Topic 2: Social and Opinion.\n",
    "This is shown from the words like 'people' and 'think' \n",
    "\n",
    "Topic 3: Politic and Government. \n",
    "Words like 'law', 'state', and 'president' are together.\n",
    "\n",
    "Topic 4: Sports. \n",
    "This can be seen in the words like 'season', 'player', and 'league'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b4137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3864c5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
